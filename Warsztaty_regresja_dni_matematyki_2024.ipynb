{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9db9ab",
   "metadata": {},
   "source": [
    "Jeśli nie masz pobranego pakietu z listy `Imports`, to odkomentuj linijkę z nazwą pakietu z poniższej komórki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba41db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install warnings\n",
    "# !pip install scikit-learn # jeśli nie masz zainstalowanego pakietu sklearn\n",
    "# !pip install xgboost\n",
    "# !pip install prophet # tylko do zadania 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b60aa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89486cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obliczenia numeryczne, tabele i wykresy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# podział danych na zbiór testowy i zbiór treningowy; walidacja krzyżowa\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# skalowanie\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# błędy i jakość predykcji\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# modele regresyjne\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d084f",
   "metadata": {},
   "source": [
    "# Praktyczne wskazówki:\n",
    "## I. Przygotowanie danych\n",
    "Aby przygotować dane do modelowania warto zwrócić uwagę na następujące kwestie:\n",
    "1) Czy w danych mamy nieinformacyjne kolumny (np. identyfikator wiersza). Jeśli tak, to można je usunąć.\n",
    "2) Czy w naszych powtarzają się pewne informacje (np. mamy dwie kolumny z tą samą cechą, ale wyrażoną w innych skalach). Jeśli tak, to można je usunąć.\n",
    "3) Czy mamy obserwacje odstające. Z czym są związane? Czy ich obecność jest związana z nieprawidłową agregacją danych, czy raczej wynikają z badanego zjawiska? Czy wnoszą do analiz potrzebne informacje, czy raczej powinniśmy je usunąć? (należy rozważać z punktu widzenia analizowanego problemu!)\n",
    "4) Czy w danych występują braki? Jeśli tak, to trzeba sobie zadać pytanie, czy jest to akceptowalne z punktu widzenia wybranego modelu. Jeśli nie to można np. \n",
    "    - usunąć brakujące wartości (uwaga! wiąże się to z utratą informacji),\n",
    "    - \n",
    "5) Czy potrzebne są dodatkowe transformacje danych (w odniesieniu do wybranego modelu)? Np. standaryzacja, normalizacja, transformacje stabilizujące wariancję...\n",
    "6) Opcjonalnie: transformacja kolumn na określony typ (w szczególności przydatne w przypadku dat). \n",
    "7) Opcjonalnie: one-hot encoding zmiennych kategorycznych (więcej informacji np. pod podanym adresem: https://ekordo.pl/uczenie-maszynowe-przygotowanie-danych-kodowanie-zmiennych-kategorialnych/)\n",
    "\n",
    "## II. Analiza danych\n",
    "Powinna się skupiać na wydobyciu najważniejszych informacji z danych. Nie ma jednego przepisu jakich technik należy użyć. Pomocne mogą być jednak następujące metody:\n",
    "- wizualizacja danych na wykresach np. wykresy rozproszenia (również 3D), wykresy mozaikowe, \n",
    "- obliczanie macierzy korelacji (np. Pearsona, Spearmana, Kendala),\n",
    "- w przypadku predykcji szeregów czasowych można zbadać również sezonowość, trend, stacjonarność szeregu.\n",
    "\n",
    "## III. Walidacja modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f97f6",
   "metadata": {},
   "source": [
    "# Zadania praktyczne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ba764",
   "metadata": {},
   "source": [
    "# Zadanie 1: Zbuduj model prognozujący sprzedaż na podstawie pieniędzy wydanych na różne platformy marketingowe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fd75a",
   "metadata": {},
   "source": [
    "1) Pobierz dane ze strony: https://www.kaggle.com/datasets/ashydv/advertising-dataset\n",
    "i umieść w tym samym folderze, co ten notatnik. Wczytaj dane za pomocą polecenia:\n",
    "```\n",
    "data = pd.read_csv(\"nazwa_pliku.csv\").\n",
    "```\n",
    "Załóżmy, że chcemy przewidywać sprzedaż (kolumna `Sales`) na podstawie kwoty wydanej na reklamy w telewizji (`TV`) (określającej . Przypisz te dane za pomocą polecenia:\n",
    "```\n",
    "X = data[\"TV\"]\n",
    "y = data[\"SALES\"]\n",
    "```\n",
    "2) Sprawdź czy w danych występują brakujące wartości.\n",
    "\n",
    "\n",
    "3) Przedstaw dane na wykresie rozproszenia. Czy zależność między danymi jest widoczna? Czy jest liniowa? A może monotoniczna? Czy są widoczne obserwacje, które skrajnie odstają?\n",
    "\n",
    "\n",
    "4) Oblicz korelację pomiędzy zmiennymi. Czy istnieje zależność między danymi?\n",
    "    \n",
    "    \n",
    "5) Jak myślisz, który model będzie odpowiedni do rozwiązania problemu? Spróbuj przetestować dowolny model z zaimportowanych: LinearRegression, Ridge, Lasso, ElasticNet, RandomForestRegressor, XGBRegressor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a777d6",
   "metadata": {},
   "source": [
    "##### Wczytywanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ...\n",
    "X    = ...\n",
    "y    = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32befd8e",
   "metadata": {},
   "source": [
    "##### Sprawdzenie czy w danych występują brakujące wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6058a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13f8cabb",
   "metadata": {},
   "source": [
    "##### Wykres rozproszenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40179180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ee8489d",
   "metadata": {},
   "source": [
    "#### Współczynniki korelacji\n",
    "\n",
    "Do jego obliczenia możesz użyć np. \n",
    "   - funkcji ze scipy.stats (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html, \n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)\n",
    "   - polecenia .corr(method=...) z pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030a292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd749ce",
   "metadata": {},
   "source": [
    "#### Krok opcjonalny\n",
    "Opcjonalnie (jeśli jest taka potrzeba) możesz pomyśleć o transformacji danych lub o standaryzacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e216f1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7f6ae47",
   "metadata": {},
   "source": [
    "#### Podział na zbiór testowy i zbiór treningowy\n",
    "Ustaw `test_size` z przedziału (0, 1). Jest to odsetek obserwacji, które zostaną przydzielone do zbioru testowego. Pozostałe posłużą do dopasowania modelu (tzw. zbiór treningowy). Często ustala się `test_size` na 0.2 lub 0.3. W skrajnych przypadkach na 0.5 (ale z reguły więcej danych wybiera się do zbioru treningowego)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963630c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = ... \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b2785",
   "metadata": {},
   "source": [
    "####  Dopasowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2ac5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7c2431b",
   "metadata": {},
   "source": [
    "#### Walidacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca20043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70176392",
   "metadata": {},
   "source": [
    "# Zadanie 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8cb8d",
   "metadata": {},
   "source": [
    "W poprzednim zadaniu skupiliśmy się na sformułowaniu zależności między zmienną niezależną, a zmienną zależną. W tym zadaniu skupimy się na problemie, w którym mamy do dyspozycji więcej zmiennych objaśniających. Zmienną, której wartość chcemy przewidywać jest średnia cena awokado.\n",
    "- Pobierz dane ze strony: https://www.kaggle.com/datasets/timmate/avocado-prices-2020\n",
    "- Wykonaj ich preprocessing oraz przeprowadź analizę zależności. \n",
    "- Wybierz cechy, które chcesz uwzględnić w modelu. \n",
    "- Wybierz przynajmniej dwa modele, które chcesz wykorzystać do predykcji zmiennej objaśnianej. Jeśli to konieczne wykonaj odpowiednie transformacje danych.\n",
    "- Porównaj ze sobą wybrane modele, wykorzystując zaprezentowane metryki. Jeśli w modelach używałeś/używałaś innej liczby zmiennych objaśniających, wówczas rozważ użycie r_squared_adjusted zamiast r_squared, ponieważ pierwszy z nich uwzglenia ilość zmiennych używanych w modelu do wyznaczania jego skuteczności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5ae4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26a06e33",
   "metadata": {},
   "source": [
    "# Zadanie 3*\n",
    "W zadaniu 1 oraz zadaniu 2 rozważaliśmy modele, które nie uwzględniały zależności od czasu. Wyobraź sobie, że próbujesz przewidzieć przyszłą liczbę pasażerów pewnej linii lotniczej. \n",
    "- Pobierz plik `AirPassengers.csv` ze strony: https://www.kaggle.com/datasets/rakannimer/air-passengers\n",
    "- Przygotuj dane do modelowania.\n",
    "- Przeanalizuj dane.\n",
    "- Podziel dane na zbiór testowy oraz zbiór treningowy względem indeksu czasowego (uwaga! nie można użyć polecenia `train_test_split`, ponieważ wybiera losowe indeksy danych do zbioru testowego i treningowego. Zamiast tego użyj operacji na listach/wektorach lub podziel dane bezpośrednio w DataFrame). Wykonaj zadanie w taki sposób, aby w zbiorze testowym znalazło się ostatnie 12 miesięcy z danych.\n",
    "- Spróbuj zamodelować dane za pomocą modelu Prophet (https://pypi.org/project/prophet/) lub innego modelu, który znasz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7a5588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
